import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from tqdm import tqdm
import pickle
import sys
import os
import re
import random
import numpy as np  # å¼•å…¥numpyç”¨äºå›ºå®šç§å­

# --- å…¨å±€æ ¸å¿ƒé…ç½® ---
NUM_THREADS = 4
BATCH_SIZE = 64
LR = 1e-4            # å­¦ä¹ ç‡
EPOCHS = 50          # è®­ç»ƒè½®æ•°
MAX_LEN = 150        # æœ€å¤§è·¯å¾„é•¿åº¦
MODEL_PATH = "movie_model.pth"
VOCAB_PATH = "vocab.pkl"
DATA_FILE = "train_data.txt"
SEED = 42            # ğŸ² å›ºå®šéšæœºç§å­

# --- é¢„æµ‹/è°ƒè¯•é…ç½® ---
DEBUG_MODE = True    # å¼€å¯åæ˜¾ç¤ºå…¨è·¯å¾„æ‰€æœ‰å­—ç¬¦å¾—åˆ†
THRESHOLD = 0.2      # æ ¸å¿ƒåˆ¤å®šé˜ˆå€¼
SMOOTH_VAL = 0.05    # è¾…åŠ©åˆ¤å®šé˜ˆå€¼ï¼ˆç”¨äºæ•‘å›ä¸­é—´å­—ç¬¦ï¼‰

# å¿…é¡»åœ¨ import torch ä¹‹åç«‹å³è®¾ç½®
torch.set_num_threads(NUM_THREADS)

# --- ğŸ› ï¸ è¾…åŠ©å‡½æ•°ï¼šå›ºå®šéšæœºç§å­ ---
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    # ä¿è¯cudnnå¯å¤ç°æ€§ï¼ˆä¼šé™ä½ä¸€ç‚¹é€Ÿåº¦ï¼Œä½†åœ¨cpuä¸Šæ— å½±å“ï¼‰
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# --- æ¨¡å‹ç»“æ„å®šä¹‰ ---
class FilmExtractor(nn.Module):
    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.gru = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 2, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        embedded = self.embedding(x)
        gru_out, _ = self.gru(embedded)
        return self.fc(gru_out).squeeze(-1)

# --- æ•°æ®é›†å®šä¹‰ ---
class MovieDataset(Dataset):
    def __init__(self, lines, char_to_idx, max_len=MAX_LEN):
        self.samples = []
        skipped_count = 0
        
        for line in lines:
            line = line.strip()
            if '#' not in line: continue
            input_path, target_name = line.rsplit('#', 1)
            target_name = target_name.strip()
            
            escaped_target = re.escape(target_name)
            pattern = escaped_target.replace(r'\ ', r'[._\s]+')
            match = re.search(pattern, input_path, re.IGNORECASE)
            
            if match:
                start_idx = match.start()
                end_idx = match.end()
                
                input_ids = [char_to_idx.get(c, 1) for c in input_path[:max_len]]
                labels = [0.0] * len(input_ids)
                
                limit = min(end_idx, max_len)
                for i in range(start_idx, limit):
                    labels[i] = 1.0
                
                pad_len = max_len - len(input_ids)
                self.samples.append((
                    torch.tensor(input_ids + [0] * pad_len), 
                    torch.tensor(labels + [0.0] * pad_len)
                ))
            else:
                skipped_count += 1

        if skipped_count > 0:
            print(f"âš ï¸ è·³è¿‡äº† {skipped_count} æ¡æ— æ³•åŒ¹é…æ ‡ç­¾çš„æ•°æ®ã€‚")

    def __len__(self): return len(self.samples)
    def __getitem__(self, idx): return self.samples[idx]

# --- ğŸ› ï¸ è¾…åŠ©å‡½æ•°ï¼šéªŒè¯é›†è®¡ç®— ---
def validate_one_epoch(model, loader, criterion):
    model.eval()
    v_loss = 0
    with torch.no_grad():
        for vx, vy in loader:
            pred = model(vx)
            loss = criterion(pred, vy)
            v_loss += loss.item()
    return v_loss / len(loader) if len(loader) > 0 else 0

# --- è®­ç»ƒé€»è¾‘ ---
def run_train():
    # è®¾ç½®å…¨å±€ç§å­ï¼Œä¿è¯åç»­ DataLoader shuffle ç­‰è¡Œä¸ºä¸€è‡´
    set_seed(SEED)
    print(f"ğŸ”’ éšæœºç§å­å·²å›ºå®šä¸º: {SEED}")

    if not os.path.exists(DATA_FILE): 
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {DATA_FILE}"); return
        
    with open(DATA_FILE, 'r', encoding='utf-8') as f: 
        lines = f.readlines()
    
    if os.path.exists(VOCAB_PATH):
        with open(VOCAB_PATH, 'rb') as f: char_to_idx = pickle.load(f)
        print("â„¹ï¸ å·²åŠ è½½ç°æœ‰è¯è¡¨ã€‚")
    else:
        raw_paths = [l.split('#')[0] for l in lines if '#' in l]
        all_chars = set("".join(raw_paths))
        char_to_idx = {c: i+2 for i, c in enumerate(sorted(list(all_chars)))}
        char_to_idx['<PAD>'], char_to_idx['<UNK>'] = 0, 1
        with open(VOCAB_PATH, 'wb') as f: pickle.dump(char_to_idx, f)
        print(f"ğŸ†• å·²åˆ›å»ºæ–°è¯è¡¨ï¼ŒåŒ…å« {len(char_to_idx)} ä¸ªå­—ç¬¦ã€‚")

    dataset = MovieDataset(lines, char_to_idx)
    if len(dataset) < 2:
        print("âŒ æœ‰æ•ˆæ ·æœ¬æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒã€‚"); return

    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    if val_size < 1: train_size -= 1; val_size += 1
    
    # ä½¿ç”¨å›ºå®šç§å­çš„ Generator è¿›è¡Œæ•°æ®é›†åˆ‡åˆ†
    # è¿™æ ·æ¯æ¬¡è¿è¡Œè„šæœ¬ï¼Œåˆ†åˆ° train å’Œ val çš„æ•°æ®æ˜¯å®Œå…¨å›ºå®šçš„
    split_generator = torch.Generator().manual_seed(SEED)
    train_ds, val_ds = random_split(dataset, [train_size, val_size], generator=split_generator)
    
    # DataLoader çš„ shuffle=True ä¹Ÿä¼šå—åˆ°å…¨å±€ torch.manual_seed çš„å½±å“
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)

    model = FilmExtractor(len(char_to_idx))
    criterion = nn.BCELoss()
    optimizer = optim.AdamW(model.parameters(), lr=LR)
    
    # åˆå§‹åŒ– best_val_loss é€»è¾‘
    best_val_loss = float('inf')

    if os.path.exists(MODEL_PATH):
        print(f"ğŸ”„ æ£€æµ‹åˆ°ç°æœ‰æ¨¡å‹ï¼ŒåŠ è½½æƒé‡ä»¥ LR={LR} ç»§ç»­å¾®è°ƒ...")
        model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
        
        # åœ¨å¼€å§‹è®­ç»ƒå¾ªç¯å‰ï¼Œå…ˆè®¡ç®—ä¸€æ¬¡å½“å‰æ¨¡å‹çš„éªŒè¯é›† Loss
        print("ğŸ“Š æ­£åœ¨è®¡ç®—å½“å‰æ¨¡å‹çš„åˆå§‹éªŒè¯é›† Loss (åŸºå‡†çº¿)...")
        initial_val_loss = validate_one_epoch(model, val_loader, criterion)
        best_val_loss = initial_val_loss # å°†èµ·ç‚¹è®¾ä¸ºå½“å‰æ¨¡å‹æ°´å¹³
        print(f"âœ… å½“å‰æ¨¡å‹åŸºå‡† Loss: {best_val_loss:.4f}")
    else:
        print("ğŸ†• æœªæ£€æµ‹åˆ°æ¨¡å‹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")

    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ | æ ·æœ¬æ•°: {len(dataset)} | è®­ç»ƒé›†: {len(train_ds)} | éªŒè¯é›†: {len(val_ds)}")
    
    try:
        for epoch in range(EPOCHS):
            model.train()
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1:02d}")
            for x, y in pbar:
                optimizer.zero_grad()
                pred = model(x)
                loss = criterion(pred, y)
                loss.backward()
                optimizer.step()
                pbar.set_postfix(loss=f"{loss.item():.4f}")
            
            # ä½¿ç”¨å°è£…å¥½çš„éªŒè¯å‡½æ•°
            avg_val_loss = validate_one_epoch(model, val_loader, criterion)
            
            # åªæœ‰å½“ Loss ç¡®å®æ¯”ä¹‹å‰çš„ï¼ˆåŒ…æ‹¬åˆšåŠ è½½è¿›æ¥çš„ï¼‰æ›´ä½æ—¶ï¼Œæ‰ä¿å­˜
            if avg_val_loss < best_val_loss:
                print(f" âœ¨ Loss ä¼˜åŒ– ({best_val_loss:.4f} -> {avg_val_loss:.4f})ï¼Œæ¨¡å‹å·²æ›´æ–°ã€‚")
                best_val_loss = avg_val_loss
                torch.save(model.state_dict(), MODEL_PATH)
            else:
                print(f" â³ éªŒè¯é›† Loss: {avg_val_loss:.4f} (æœªæå‡ï¼Œæœ€ä½³: {best_val_loss:.4f})")
                
    except KeyboardInterrupt: print("\nğŸ›‘ ç”¨æˆ·æ‰‹åŠ¨åœæ­¢è®­ç»ƒã€‚")

# --- é¢„æµ‹é€»è¾‘ ---
def run_predict(path):
    if not os.path.exists(MODEL_PATH) or not os.path.exists(VOCAB_PATH):
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æˆ–è¯è¡¨æ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œè®­ç»ƒã€‚"); return

    with open(VOCAB_PATH, 'rb') as f: char_to_idx = pickle.load(f)
    model = FilmExtractor(len(char_to_idx))
    model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
    model.eval()

    input_ids = [char_to_idx.get(c, 1) for c in path[:MAX_LEN]]
    padded = input_ids + [0] * (MAX_LEN - len(input_ids))
    
    with torch.no_grad():
        probs = model(torch.tensor([padded]))[0][:len(path)].numpy()

    if DEBUG_MODE:
        print(f"\n{'='*65}")
        print(f"{'ç´¢å¼•':<4} | {'å­—ç¬¦':<4} | {'åˆ†å€¼':<15} | çŠ¶æ€")
        print("-" * 65)
        for i, p in enumerate(probs):
            status = "âœ… [é€‰ä¸­]" if p > THRESHOLD else "   [æ’é™¤]"
            print(f"{i:<4} | {path[i]:<4} | {p:.10f} | {status}")
        print(f"{'='*65}\n")

    res_list = []
    for i, p in enumerate(probs):
        is_high = p > THRESHOLD
        is_bridge = False
        if not is_high and p > SMOOTH_VAL:
            left_high = probs[i-1] > THRESHOLD if i > 0 else False
            right_high = probs[i+1] > THRESHOLD if i < len(probs)-1 else False
            if left_high and right_high:
                is_bridge = True
        
        if is_high or is_bridge:
            res_list.append(path[i])
    
    raw_result = "".join(res_list)
    clean_result = raw_result.replace('.', ' ').replace('_', ' ')
    clean_result = re.sub(r'\s+', ' ', clean_result)
    clean_result = clean_result.strip("/()# â€œâ€.-")

    if DEBUG_MODE: 
        print(f"ğŸ“¥ æå–åŸæ–‡: {raw_result}")
        print(f"âœ… æœ€ç»ˆç»“æœ: {clean_result}\n")
    else: 
        print(clean_result)

# --- å…¥å£æ§åˆ¶ ---
if __name__ == "__main__":
    if len(sys.argv) > 1:
        run_predict(sys.argv[1])
    else:
        run_train()
        