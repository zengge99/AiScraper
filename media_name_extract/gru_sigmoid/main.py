import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from tqdm import tqdm
import pickle
import sys
import os
import re

# --- å…¨å±€æ ¸å¿ƒé…ç½® ---
NUM_THREADS = 4
BATCH_SIZE = 64
LR = 1e-4            # å­¦ä¹ ç‡
EPOCHS = 50          # è®­ç»ƒè½®æ•°
MAX_LEN = 150        # æœ€å¤§è·¯å¾„é•¿åº¦
MODEL_PATH = "movie_model.pth"
VOCAB_PATH = "vocab.pkl"
DATA_FILE = "train_data.txt"

# --- é¢„æµ‹/è°ƒè¯•é…ç½® ---
DEBUG_MODE = True    # å¼€å¯åæ˜¾ç¤ºå…¨è·¯å¾„æ‰€æœ‰å­—ç¬¦å¾—åˆ†
THRESHOLD = 0.2      # æ ¸å¿ƒåˆ¤å®šé˜ˆå€¼
SMOOTH_VAL = 0.05    # è¾…åŠ©åˆ¤å®šé˜ˆå€¼ï¼ˆç”¨äºæ•‘å›ä¸­é—´å­—ç¬¦ï¼‰

# å¿…é¡»åœ¨ import torch ä¹‹åç«‹å³è®¾ç½®
torch.set_num_threads(NUM_THREADS)

# --- æ¨¡å‹ç»“æ„å®šä¹‰ ---
class FilmExtractor(nn.Module):
    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.gru = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 2, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        embedded = self.embedding(x)
        gru_out, _ = self.gru(embedded)
        return self.fc(gru_out).squeeze(-1)

# --- æ•°æ®é›†å®šä¹‰ ---
class MovieDataset(Dataset):
    def __init__(self, lines, char_to_idx, max_len=MAX_LEN):
        self.samples = []
        skipped_count = 0
        
        for line in lines:
            line = line.strip()
            if '#' not in line: continue
            input_path, target_name = line.rsplit('#', 1)
            target_name = target_name.strip()
            
            # æ„é€ æ­£åˆ™ï¼šå…è®¸ç›®æ ‡åä¸­çš„ç©ºæ ¼å¯¹åº”è·¯å¾„ä¸­çš„ "." "_" æˆ– " "
            # ä¾‹å¦‚ç›®æ ‡ "Transformers The" -> æ­£åˆ™ "Transformers[._\s]+The"
            escaped_target = re.escape(target_name)
            pattern = escaped_target.replace(r'\ ', r'[._\s]+')
            
            # åœ¨è·¯å¾„ä¸­æœç´¢åŒ¹é…é¡¹ (å¿½ç•¥å¤§å°å†™)
            match = re.search(pattern, input_path, re.IGNORECASE)
            
            if match:
                start_idx = match.start()
                end_idx = match.end()
                
                # æ„å»º Label
                input_ids = [char_to_idx.get(c, 1) for c in input_path[:max_len]]
                labels = [0.0] * len(input_ids)
                
                # åªæœ‰åŒ¹é…åˆ°çš„éƒ¨åˆ†æ ‡ä¸º 1.0
                limit = min(end_idx, max_len)
                for i in range(start_idx, limit):
                    labels[i] = 1.0
                
                # Padding
                pad_len = max_len - len(input_ids)
                self.samples.append((
                    torch.tensor(input_ids + [0] * pad_len), 
                    torch.tensor(labels + [0.0] * pad_len)
                ))
            else:
                # å¦‚æœå®Œå…¨åŒ¹é…ä¸åˆ°ï¼ˆæ¯”å¦‚æ•°æ®æ ‡æ³¨é”™äº†ï¼‰ï¼Œåˆ™è·³è¿‡
                skipped_count += 1
            # --- æ ¸å¿ƒä¿®æ”¹ç»“æŸ ---

        if skipped_count > 0:
            print(f"âš ï¸ è·³è¿‡äº† {skipped_count} æ¡æ— æ³•åŒ¹é…æ ‡ç­¾çš„æ•°æ®ã€‚")

    def __len__(self): return len(self.samples)
    def __getitem__(self, idx): return self.samples[idx]

# --- è®­ç»ƒé€»è¾‘ ---
def run_train():
    if not os.path.exists(DATA_FILE): 
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {DATA_FILE}"); return
        
    with open(DATA_FILE, 'r', encoding='utf-8') as f: 
        lines = f.readlines()
    
    # æ„å»ºè¯è¡¨
    if os.path.exists(VOCAB_PATH):
        with open(VOCAB_PATH, 'rb') as f: char_to_idx = pickle.load(f)
        print("â„¹ï¸ å·²åŠ è½½ç°æœ‰è¯è¡¨ã€‚")
    else:
        # åªç»Ÿè®¡ '#' å·¦è¾¹çš„å­—ç¬¦ï¼ˆå³è·¯å¾„éƒ¨åˆ†ï¼‰
        raw_paths = [l.split('#')[0] for l in lines if '#' in l]
        all_chars = set("".join(raw_paths))
        char_to_idx = {c: i+2 for i, c in enumerate(sorted(list(all_chars)))}
        char_to_idx['<PAD>'], char_to_idx['<UNK>'] = 0, 1
        with open(VOCAB_PATH, 'wb') as f: pickle.dump(char_to_idx, f)
        print(f"ğŸ†• å·²åˆ›å»ºæ–°è¯è¡¨ï¼ŒåŒ…å« {len(char_to_idx)} ä¸ªå­—ç¬¦ã€‚")

    dataset = MovieDataset(lines, char_to_idx)
    if len(dataset) < 2:
        print("âŒ æœ‰æ•ˆæ ·æœ¬æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒã€‚"); return

    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    # ä¿®å¤ï¼šé˜²æ­¢æ•°æ®é›†è¿‡å°å¯¼è‡´ val_size ä¸º 0
    if val_size < 1: train_size -= 1; val_size += 1
        
    train_ds, val_ds = random_split(dataset, [train_size, val_size])
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)

    model = FilmExtractor(len(char_to_idx))
    if os.path.exists(MODEL_PATH):
        print(f"ğŸ”„ æ£€æµ‹åˆ°ç°æœ‰æ¨¡å‹ï¼ŒåŠ è½½æƒé‡ä»¥ LR={LR} ç»§ç»­å¾®è°ƒ...")
        model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
    
    optimizer = optim.AdamW(model.parameters(), lr=LR)
    criterion = nn.BCELoss()
    best_val_loss = float('inf') 

    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ | æ ·æœ¬æ•°: {len(dataset)} (Skipå¤±è´¥æ ·æœ¬)")
    try:
        for epoch in range(EPOCHS):
            model.train()
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1:02d}")
            for x, y in pbar:
                optimizer.zero_grad()
                pred = model(x)
                loss = criterion(pred, y)
                loss.backward()
                optimizer.step()
                pbar.set_postfix(loss=f"{loss.item():.4f}")
            
            model.eval()
            v_loss = 0
            with torch.no_grad():
                for vx, vy in val_loader:
                    v_loss += criterion(model(vx), vy).item()
            
            avg_val_loss = v_loss / len(val_loader)
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                torch.save(model.state_dict(), MODEL_PATH)
                print(f" âœ¨ éªŒè¯é›† Loss æå‡è‡³ {avg_val_loss:.4f}ï¼Œæ¨¡å‹å·²ä¿å­˜ã€‚")
            else:
                print(f" â³ éªŒè¯é›† Loss: {avg_val_loss:.4f} (æœªæå‡)")
    except KeyboardInterrupt: print("\nğŸ›‘ ç”¨æˆ·æ‰‹åŠ¨åœæ­¢è®­ç»ƒã€‚")

# --- é¢„æµ‹é€»è¾‘ (æ ¸å¿ƒä¿®æ”¹ï¼šåå¤„ç†) ---
def run_predict(path):
    if not os.path.exists(MODEL_PATH) or not os.path.exists(VOCAB_PATH):
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æˆ–è¯è¡¨æ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œè®­ç»ƒã€‚"); return

    with open(VOCAB_PATH, 'rb') as f: char_to_idx = pickle.load(f)
    model = FilmExtractor(len(char_to_idx))
    model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
    model.eval()

    # é¢„å¤„ç†ï¼šæˆªæ–­å’Œè½¬æ¢ ID
    input_ids = [char_to_idx.get(c, 1) for c in path[:MAX_LEN]]
    padded = input_ids + [0] * (MAX_LEN - len(input_ids))
    
    with torch.no_grad():
        probs = model(torch.tensor([padded]))[0][:len(path)].numpy()

    if DEBUG_MODE:
        print(f"\n{'='*65}")
        print(f"{'ç´¢å¼•':<4} | {'å­—ç¬¦':<4} | {'åˆ†å€¼ (10ä½å°æ•°)':<15} | çŠ¶æ€")
        print("-" * 65)
        for i, p in enumerate(probs):
            status = "âœ… [é€‰ä¸­]" if p > THRESHOLD else "   [æ’é™¤]"
            print(f"{i:<4} | {path[i]:<4} | {p:.10f} | {status}")
        print(f"{'='*65}\n")

    # å¢å¼ºå‹æå–é€»è¾‘ (æ¡¥æ¢é€»è¾‘)
    res_list = []
    for i, p in enumerate(probs):
        is_high = p > THRESHOLD
        is_bridge = False
        if not is_high and p > SMOOTH_VAL:
            left_high = probs[i-1] > THRESHOLD if i > 0 else False
            right_high = probs[i+1] > THRESHOLD if i < len(probs)-1 else False
            if left_high and right_high:
                is_bridge = True
        
        if is_high or is_bridge:
            res_list.append(path[i])
    
    raw_result = "".join(res_list)
    
    # ç›®çš„ï¼šå°† Transformers.The.Last.Knight è½¬æ¢ä¸º Transformers The Last Knight
    # A. æ›¿æ¢ç‚¹å’Œä¸‹åˆ’çº¿ä¸ºç©ºæ ¼
    clean_result = raw_result.replace('.', ' ').replace('_', ' ')
    
    # B. å†æ¬¡æ­£åˆ™æ¸…æ´—ï¼šæŠŠè¿ç»­çš„ç©ºæ ¼å˜æˆå•ä¸ªç©ºæ ¼
    clean_result = re.sub(r'\s+', ' ', clean_result)
    
    # C. å»æ‰é¦–å°¾å¯èƒ½æ®‹ç•™çš„éå­—æ¯ç¬¦å· (å¦‚ / ( ) - ç­‰)
    clean_result = clean_result.strip("/()# â€œâ€.-")

    if DEBUG_MODE: 
        print(f"ğŸ“¥ æå–åŸæ–‡: {raw_result}")
        print(f"âœ… æœ€ç»ˆç»“æœ: {clean_result}\n")
    else: 
        print(clean_result)

# --- å…¥å£æ§åˆ¶ ---
if __name__ == "__main__":
    if len(sys.argv) > 1:
        run_predict(sys.argv[1])
    else:
        # ä¸å¸¦å‚æ•°ï¼šè®­ç»ƒæ¨¡å¼
        run_train()