import os
import torch
import argparse
import warnings
from torch.utils.data import DataLoader
from transformers import BertTokenizer, get_linear_schedule_with_warmup
from torch.optim import AdamW
from model import MediaNameExtractor
from dataset import SimpleMediaDataset

# å¿½ç•¥å†—ä½™è­¦å‘Š
warnings.filterwarnings("ignore")

# å…¨å±€é…ç½®ï¼ˆVPS CPUé€‚é…ï¼‰
DEVICE = torch.device("cpu")
BATCH_SIZE = 1
MAX_PATH_LEN = 128
MAX_NAME_LEN = 32
LR = 3e-5
SAVE_PATH = "best_media_model.pt"  # æœ€ä¼˜æ¨¡å‹ä¿å­˜è·¯å¾„
LOSS_RECORD_PATH = "best_loss.txt" # è®°å½•æœ€ä¼˜éªŒè¯æŸå¤±çš„æ–‡ä»¶

def load_best_loss():
    """åŠ è½½å†å²æœ€ä¼˜éªŒè¯æŸå¤±"""
    if os.path.exists(LOSS_RECORD_PATH):
        try:
            with open(LOSS_RECORD_PATH, "r", encoding="utf-8") as f:
                loss = float(f.read().strip())
            return loss
        except:
            return float("inf")
    return float("inf")

def save_best_loss(loss):
    """ä¿å­˜æœ€æ–°æœ€ä¼˜éªŒè¯æŸå¤±"""
    with open(LOSS_RECORD_PATH, "w", encoding="utf-8") as f:
        f.write(f"{loss:.6f}")

def train(args):
    """è®­ç»ƒæ¨¡å‹ï¼šè‡ªåŠ¨åŠ è½½æœ€ä¼˜æ¨¡å‹+è¯»å–æœ€æ–°æ•°æ®"""
    # 1. åˆå§‹åŒ–åˆ†è¯å™¨
    tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
    
    # 2. è¯»å–æœ€æ–°çš„è®­ç»ƒ/éªŒè¯æ•°æ®ï¼ˆæ¯æ¬¡è®­ç»ƒéƒ½ä¼šé‡æ–°è¯»å–ï¼Œè‡ªåŠ¨ç”¨æ–°æ•°æ®ï¼‰
    print(f"ğŸ“– æ­£åœ¨è¯»å–æœ€æ–°æ•°æ®ï¼šè®­ç»ƒé›†={args.train_data} | éªŒè¯é›†={args.dev_data}")
    train_dataset = SimpleMediaDataset(args.train_data, tokenizer, MAX_PATH_LEN, MAX_NAME_LEN)
    dev_dataset = SimpleMediaDataset(args.dev_data, tokenizer, MAX_PATH_LEN, MAX_NAME_LEN)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    # 3. åˆå§‹åŒ–æ¨¡å‹ï¼ˆè‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½æœ€ä¼˜æ¨¡å‹ï¼‰
    model = MediaNameExtractor().to(DEVICE)
    best_dev_loss = load_best_loss()  # åŠ è½½å†å²æœ€ä¼˜æŸå¤±
    model_loaded = False
    
    if os.path.exists(SAVE_PATH):
        try:
            model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))
            model_loaded = True
            print(f"âœ… è‡ªåŠ¨åŠ è½½æœ€ä¼˜æ¨¡å‹ï¼š{SAVE_PATH}ï¼Œå†å²æœ€ä¼˜éªŒè¯æŸå¤±ï¼š{best_dev_loss:.4f}")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ¨¡å‹å¤±è´¥ï¼Œå°†ä»å¤´è®­ç»ƒï¼š{str(e)}")
            best_dev_loss = float("inf")
    else:
        print("ğŸš€ æœªæ‰¾åˆ°æœ€ä¼˜æ¨¡å‹ï¼Œå°†ä»å¤´è®­ç»ƒ")

    # 4. ä¼˜åŒ–å™¨é…ç½®
    optimizer = AdamW(
        model.parameters(),
        lr=LR,
        eps=1e-8,
        weight_decay=0.01
    )
    total_steps = len(train_loader) * args.epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=0,
        num_training_steps=total_steps
    )

    # 5. è®­ç»ƒå¾ªç¯ï¼ˆä½¿ç”¨æœ€æ–°æ•°æ®ç»§ç»­è®­ç»ƒï¼‰
    print(f"\n===== å¼€å§‹è®­ç»ƒï¼ˆCPUæ¨¡å¼ï¼‰=====")
    print(f"è®­ç»ƒæ•°æ®é‡ï¼š{len(train_dataset)} | éªŒè¯æ•°æ®é‡ï¼š{len(dev_dataset)}")
    print(f"è®­ç»ƒè½®æ•°ï¼š{args.epochs} | å·²åŠ è½½æ¨¡å‹ï¼š{model_loaded}")

    for epoch in range(args.epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        for batch in train_loader:
            path_ids = batch["path_ids"].to(DEVICE)
            path_mask = batch["path_mask"].to(DEVICE)
            name_ids = batch["name_ids"].to(DEVICE)
            name_mask = batch["name_mask"].to(DEVICE)

            # å‰å‘ä¼ æ’­
            outputs = model(path_ids, path_mask, name_ids, name_mask)
            loss = outputs["loss"]
            train_loss += loss.item()

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

        # éªŒè¯é˜¶æ®µï¼ˆç”¨æœ€æ–°éªŒè¯æ•°æ®è¯„ä¼°ï¼‰
        model.eval()
        dev_loss = 0.0
        with torch.no_grad():
            for batch in dev_loader:
                path_ids = batch["path_ids"].to(DEVICE)
                path_mask = batch["path_mask"].to(DEVICE)
                name_ids = batch["name_ids"].to(DEVICE)
                name_mask = batch["name_mask"].to(DEVICE)
                outputs = model(path_ids, path_mask, name_ids, name_mask)
                dev_loss += outputs["loss"].item()

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_train_loss = train_loss / len(train_loader)
        avg_dev_loss = dev_loss / len(dev_loader)

        # ä¿å­˜æ›´ä¼˜çš„æ¨¡å‹å’ŒæŸå¤±ï¼ˆåªæœ‰éªŒè¯æŸå¤±æ›´ä½æ—¶æ‰æ›´æ–°ï¼‰
        if avg_dev_loss < best_dev_loss:
            best_dev_loss = avg_dev_loss
            torch.save(model.state_dict(), SAVE_PATH)
            save_best_loss(best_dev_loss)
            print(f"[Epoch {epoch+1}] ğŸ‰ éªŒè¯æŸå¤±ä¸‹é™ ({best_dev_loss:.4f})ï¼Œæ›´æ–°æœ€ä¼˜æ¨¡å‹")
        else:
            print(f"[Epoch {epoch+1}] âŒ éªŒè¯æŸå¤±æœªä¸‹é™ (å½“å‰ï¼š{avg_dev_loss:.4f} | æœ€ä¼˜ï¼š{best_dev_loss:.4f})")

        print(f"[Epoch {epoch+1}] è®­ç»ƒæŸå¤±ï¼š{avg_train_loss:.4f} | éªŒè¯æŸå¤±ï¼š{avg_dev_loss:.4f}")

    print(f"\n===== è®­ç»ƒå®Œæˆï¼æœ€ä¼˜æ¨¡å‹ï¼š{SAVE_PATH} =====")

def infer(args):
    """æ¨ç†ï¼šä»è·¯å¾„æå–åç§°"""
    # 1. æ£€æŸ¥æ¨¡å‹
    if not os.path.exists(SAVE_PATH):
        print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°æ¨¡å‹ {SAVE_PATH}ï¼Œè¯·å…ˆè®­ç»ƒï¼")
        return

    # 2. åŠ è½½æ¨¡å‹
    model = MediaNameExtractor().to(DEVICE)
    model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))
    model.eval()

    # 3. æå–åç§°
    path = args.path
    print(f"åŸå§‹è·¯å¾„ï¼š{path}")
    name = model.extract_name(path)
    print(f"æå–çš„å½±è§†åç§°ï¼š{name}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å½±è§†åç§°æå–æ¨¡å‹ï¼ˆè‡ªåŠ¨ç»­è®­+è¯»å–æœ€æ–°æ•°æ®ï¼‰")
    subparsers = parser.add_subparsers(dest="command", help="å­å‘½ä»¤ï¼štrain / infer")

    # è®­ç»ƒå­å‘½ä»¤ï¼ˆæç®€ä½¿ç”¨ï¼Œé»˜è®¤å‚æ•°å³å¯ï¼‰
    train_parser = subparsers.add_parser("train", help="è®­ç»ƒæ¨¡å‹ï¼ˆè‡ªåŠ¨åŠ è½½æœ€ä¼˜æ¨¡å‹+æœ€æ–°æ•°æ®ï¼‰")
    train_parser.add_argument("--train_data", type=str, default="train_data.txt", help="è®­ç»ƒæ•°æ®è·¯å¾„ï¼ˆé»˜è®¤train_data.txtï¼‰")
    train_parser.add_argument("--dev_data", type=str, default="dev_data.txt", help="éªŒè¯æ•°æ®è·¯å¾„ï¼ˆé»˜è®¤dev_data.txtï¼‰")
    train_parser.add_argument("--epochs", type=int, default=5, help="æ¯æ¬¡è®­ç»ƒçš„è½®æ•°ï¼ˆé»˜è®¤5è½®ï¼Œå»ºè®®å°è½®æ•°å¤šæ¬¡è®­ï¼‰")

    # æ¨ç†å­å‘½ä»¤
    infer_parser = subparsers.add_parser("infer", help="æå–å½±è§†åç§°")
    infer_parser.add_argument("--path", type=str, required=True, help="åŸå§‹æ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()
    if args.command == "train":
        train(args)
    elif args.command == "infer":
        infer(args)
    else:
        parser.print_help()