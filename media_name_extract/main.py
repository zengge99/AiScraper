import os
import torch
import argparse
import warnings
from torch.utils.data import DataLoader
from transformers import BertTokenizer
from model import MediaNameExtractor
from dataset import SimpleMediaDataset

warnings.filterwarnings("ignore")

# å…¨å±€é…ç½®
DEVICE = torch.device("cpu")
BATCH_SIZE = 4
MAX_PATH_LEN = 128
MAX_NAME_LEN = 32
LR = 5e-5  # æé«˜å­¦ä¹ ç‡ï¼ŒåŠ é€ŸBERTå¾®è°ƒ
SAVE_PATH = "best_media_model.pt"
LOSS_RECORD_PATH = "best_loss.txt"

def load_best_loss():
    if os.path.exists(LOSS_RECORD_PATH):
        try:
            with open(LOSS_RECORD_PATH, "r", encoding="utf-8") as f:
                loss = float(f.read().strip())
            return loss
        except:
            return float("inf")
    return float("inf")

def save_best_loss(loss):
    with open(LOSS_RECORD_PATH, "w", encoding="utf-8") as f:
        f.write(f"{loss:.6f}")

def train(args):
    # åˆå§‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹
    tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
    model = MediaNameExtractor().to(DEVICE)
    
    # åŠ è½½æ•°æ®
    train_dataset = SimpleMediaDataset(args.train_data, tokenizer, MAX_PATH_LEN, MAX_NAME_LEN)
    dev_dataset = SimpleMediaDataset(args.dev_data, tokenizer, MAX_PATH_LEN, MAX_NAME_LEN)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # åŠ è½½æœ€ä¼˜æ¨¡å‹ï¼ˆå¦‚æœ‰ï¼‰
    best_dev_loss = load_best_loss()
    model_loaded = False
    if os.path.exists(SAVE_PATH):
        try:
            model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))
            model.eval()
            init_dev_loss = 0.0
            with torch.no_grad():
                for batch in dev_loader:
                    path_ids = batch["path_ids"].to(DEVICE)
                    path_mask = batch["path_mask"].to(DEVICE)
                    name_ids = batch["name_ids"].to(DEVICE)
                    name_mask = batch["name_mask"].to(DEVICE)
                    outputs = model(path_ids, path_mask, name_ids, name_mask)
                    init_dev_loss += outputs["loss"].item()
            best_dev_loss = init_dev_loss / len(dev_loader)
            model_loaded = True
            print(f"âœ… åŠ è½½æœ€ä¼˜æ¨¡å‹ï¼Œå†å²æœ€ä¼˜éªŒè¯æŸå¤±ï¼š{best_dev_loss:.4f}")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ¨¡å‹å¤±è´¥ï¼Œä»å¤´è®­ç»ƒï¼š{e}")
            best_dev_loss = float("inf")

    # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.1)
    total_steps = len(train_loader) * args.epochs
    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=total_steps)

    # è®­ç»ƒå¾ªç¯
    print(f"\n===== å¼€å§‹è®­ç»ƒ =====")
    print(f"è®­ç»ƒè½®æ•°ï¼š{args.epochs} | æ‰¹æ¬¡å¤§å°ï¼š{BATCH_SIZE} | å­¦ä¹ ç‡ï¼š{LR}")
    for epoch in range(args.epochs):
        model.train()
        train_loss_total = 0.0
        
        for batch_idx, batch in enumerate(train_loader):
            path_ids = batch["path_ids"].to(DEVICE)
            path_mask = batch["path_mask"].to(DEVICE)
            name_ids = batch["name_ids"].to(DEVICE)
            name_mask = batch["name_mask"].to(DEVICE)

            # å‰å‘+åå‘ä¼ æ’­
            outputs = model(path_ids, path_mask, name_ids, name_mask)
            loss = outputs["loss"]
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

            train_loss_total += loss.item()

            # æ‰“å°æ‰¹æ¬¡æŸå¤±
            if (batch_idx + 1) % 5 == 0:
                print(f"[Epoch {epoch+1}/{args.epochs}] Batch {batch_idx+1} | æ‰¹æ¬¡æŸå¤±ï¼š{loss.item():.4f}")

        # éªŒè¯é˜¶æ®µ
        model.eval()
        dev_loss_total = 0.0
        with torch.no_grad():
            for batch in dev_loader:
                path_ids = batch["path_ids"].to(DEVICE)
                path_mask = batch["path_mask"].to(DEVICE)
                name_ids = batch["name_ids"].to(DEVICE)
                name_mask = batch["name_mask"].to(DEVICE)
                outputs = model(path_ids, path_mask, name_ids, name_mask)
                dev_loss_total += outputs["loss"].item()

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_train_loss = train_loss_total / len(train_loader)
        avg_dev_loss = dev_loss_total / len(dev_loader)

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if avg_dev_loss < best_dev_loss:
            best_dev_loss = avg_dev_loss
            torch.save(model.state_dict(), SAVE_PATH)
            save_best_loss(best_dev_loss)
            print(f"[Epoch {epoch+1}] ğŸ‰ éªŒè¯æŸå¤±ä¸‹é™ï¼š{best_dev_loss:.4f}ï¼Œä¿å­˜æ¨¡å‹")
        else:
            print(f"[Epoch {epoch+1}] âŒ éªŒè¯æŸå¤±æœªä¸‹é™ï¼šå½“å‰={avg_dev_loss:.4f} | æœ€ä¼˜={best_dev_loss:.4f}")

        print(f"[Epoch {epoch+1}] è®­ç»ƒæŸå¤±ï¼š{avg_train_loss:.4f} | éªŒè¯æŸå¤±ï¼š{avg_dev_loss:.4f}\n")

    print(f"===== è®­ç»ƒå®Œæˆï¼æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜è‡³ {SAVE_PATH} =====")

def infer(args):
    # åŠ è½½æ¨¡å‹
    if not os.path.exists(SAVE_PATH):
        print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ {SAVE_PATH}ï¼Œè¯·å…ˆè®­ç»ƒï¼")
        return

    model = MediaNameExtractor().to(DEVICE)
    try:
        model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))
        model.eval()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
        return

    # æå–åç§°
    model.extract_name(args.path)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å½±è§†åç§°æå–æ¨¡å‹ï¼ˆè§£å†³åˆ†æ•°é›†ä¸­0.5é—®é¢˜ï¼‰")
    subparsers = parser.add_subparsers(dest="command", help="å­å‘½ä»¤ï¼štrain / infer")

    # è®­ç»ƒå‚æ•°
    train_parser = subparsers.add_parser("train", help="è®­ç»ƒæ¨¡å‹")
    train_parser.add_argument("--train_data", type=str, default="train_data.txt", help="è®­ç»ƒæ•°æ®è·¯å¾„")
    train_parser.add_argument("--dev_data", type=str, default="dev_data.txt", help="éªŒè¯æ•°æ®è·¯å¾„")
    train_parser.add_argument("--epochs", type=int, default=20, help="è®­ç»ƒè½®æ•°ï¼ˆå»ºè®®20è½®ï¼‰")

    # æ¨ç†å‚æ•°
    infer_parser = subparsers.add_parser("infer", help="æå–å½±è§†åç§°")
    infer_parser.add_argument("--path", type=str, required=True, help="å¾…æå–çš„æ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()
    if args.command == "train":
        train(args)
    elif args.command == "infer":
        infer(args)
    else:
        parser.print_help()