import os
import torch
import argparse
import warnings
from torch.utils.data import DataLoader
from transformers import BertTokenizer, get_linear_schedule_with_warmup
from torch.optim import AdamW
from model import MediaNameExtractor
from dataset import SimpleMediaDataset

# å¿½ç•¥å†—ä½™è­¦å‘Š
warnings.filterwarnings("ignore")

# å…¨å±€é…ç½®
DEVICE = torch.device("cpu")
BATCH_SIZE = 2  # å¢å¤§æ‰¹æ¬¡ï¼Œç¼“è§£è¿‡æ‹Ÿåˆ
MAX_PATH_LEN = 128
MAX_NAME_LEN = 32
LR = 1e-5  # è°ƒä½å­¦ä¹ ç‡ï¼Œé¿å…è®­ç»ƒéœ‡è¡
SAVE_PATH = "best_media_model.pt"
LOSS_RECORD_PATH = "best_loss.txt"

def load_best_loss():
    """åŠ è½½å†å²æœ€ä¼˜æŸå¤±"""
    if os.path.exists(LOSS_RECORD_PATH):
        try:
            with open(LOSS_RECORD_PATH, "r", encoding="utf-8") as f:
                loss = float(f.read().strip())
            return loss
        except:
            return float("inf")
    return float("inf")

def save_best_loss(loss):
    """ä¿å­˜æœ€æ–°æœ€ä¼˜æŸå¤±"""
    with open(LOSS_RECORD_PATH, "w", encoding="utf-8") as f:
        f.write(f"{loss:.6f}")

def train(args):
    """è®­ç»ƒæ¨¡å‹ï¼šè‡ªåŠ¨ç»­è®­+ç¼“è§£è¿‡æ‹Ÿåˆ"""
    # 1. åˆå§‹åŒ–åˆ†è¯å™¨å’Œæ•°æ®é›†
    tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
    print(f"ğŸ“– è¯»å–æ•°æ®ï¼šè®­ç»ƒé›†={args.train_data} | éªŒè¯é›†={args.dev_data}")
    train_dataset = SimpleMediaDataset(args.train_data, tokenizer, MAX_PATH_LEN, MAX_NAME_LEN)
    dev_dataset = SimpleMediaDataset(args.dev_data, tokenizer, MAX_PATH_LEN, MAX_NAME_LEN)
    
    # æ•°æ®åŠ è½½å™¨ï¼šæ‰“ä¹±+å¢å¤§æ‰¹æ¬¡
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # 2. åˆå§‹åŒ–æ¨¡å‹ï¼ˆè‡ªåŠ¨ç»­è®­ï¼‰
    model = MediaNameExtractor().to(DEVICE)
    best_dev_loss = load_best_loss()
    model_loaded = False

    if os.path.exists(SAVE_PATH):
        try:
            model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))
            # éªŒè¯åŠ è½½åçš„æ¨¡å‹æŸå¤±
            model.eval()
            init_dev_loss = 0.0
            with torch.no_grad():
                for batch in dev_loader:
                    path_ids = batch["path_ids"].to(DEVICE)
                    path_mask = batch["path_mask"].to(DEVICE)
                    name_ids = batch["name_ids"].to(DEVICE)
                    name_mask = batch["name_mask"].to(DEVICE)
                    outputs = model(path_ids, path_mask, name_ids, name_mask)
                    init_dev_loss += outputs["loss"].item()
            best_dev_loss = init_dev_loss / len(dev_loader)
            model_loaded = True
            print(f"âœ… åŠ è½½æœ€ä¼˜æ¨¡å‹ï¼Œå†å²æœ€ä¼˜éªŒè¯æŸå¤±ï¼š{best_dev_loss:.4f}")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ¨¡å‹å¤±è´¥ï¼Œä»å¤´è®­ç»ƒï¼š{str(e)}")
            best_dev_loss = float("inf")

    # 3. ä¼˜åŒ–å™¨é…ç½®ï¼ˆåŠ å…¥æƒé‡è¡°å‡ç¼“è§£è¿‡æ‹Ÿåˆï¼‰
    optimizer = AdamW(
        model.parameters(),
        lr=LR,
        eps=1e-8,
        weight_decay=0.05  # æƒé‡è¡°å‡ï¼Œç¼“è§£è¿‡æ‹Ÿåˆ
    )
    total_steps = len(train_loader) * args.epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(total_steps*0.1),  # 10%é¢„çƒ­æ­¥æ•°
        num_training_steps=total_steps
    )

    # 4. è®­ç»ƒå¾ªç¯ï¼ˆæ§åˆ¶è½®æ•°ï¼Œé¿å…è¿‡æ‹Ÿåˆï¼‰
    print(f"\n===== å¼€å§‹è®­ç»ƒ =====")
    print(f"è®­ç»ƒè½®æ•°ï¼š{args.epochs} | æ‰¹æ¬¡å¤§å°ï¼š{BATCH_SIZE} | å·²åŠ è½½æ¨¡å‹ï¼š{model_loaded}")
    
    for epoch in range(args.epochs):
        model.train()
        train_loss_total = 0.0
        
        for batch_idx, batch in enumerate(train_loader):
            # æ•°æ®è¿ç§»åˆ°è®¾å¤‡
            path_ids = batch["path_ids"].to(DEVICE)
            path_mask = batch["path_mask"].to(DEVICE)
            name_ids = batch["name_ids"].to(DEVICE)
            name_mask = batch["name_mask"].to(DEVICE)

            # å‰å‘ä¼ æ’­
            outputs = model(path_ids, path_mask, name_ids, name_mask)
            loss = outputs["loss"]
            
            # åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦è£å‰ªï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            loss.backward()
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

            train_loss_total += loss.item()

            # æ‰“å°æ‰¹æ¬¡æŸå¤±ï¼ˆç›‘æ§è®­ç»ƒçŠ¶æ€ï¼‰
            if (batch_idx + 1) % 5 == 0:
                print(f"[Epoch {epoch+1}/{args.epochs}] Batch {batch_idx+1} | æ‰¹æ¬¡æŸå¤±ï¼š{loss.item():.4f}")

        # éªŒè¯é˜¶æ®µ
        model.eval()
        dev_loss_total = 0.0
        with torch.no_grad():
            for batch in dev_loader:
                path_ids = batch["path_ids"].to(DEVICE)
                path_mask = batch["path_mask"].to(DEVICE)
                name_ids = batch["name_ids"].to(DEVICE)
                name_mask = batch["name_mask"].to(DEVICE)
                outputs = model(path_ids, path_mask, name_ids, name_mask)
                dev_loss_total += outputs["loss"].item()

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_train_loss = train_loss_total / len(train_loader)
        avg_dev_loss = dev_loss_total / len(dev_loader)

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆä»…å½“éªŒè¯æŸå¤±ä¸‹é™æ—¶ï¼‰
        if avg_dev_loss < best_dev_loss:
            best_dev_loss = avg_dev_loss
            torch.save(model.state_dict(), SAVE_PATH)
            save_best_loss(best_dev_loss)
            print(f"[Epoch {epoch+1}] ğŸ‰ éªŒè¯æŸå¤±ä¸‹é™ï¼š{best_dev_loss:.4f}ï¼Œä¿å­˜æ¨¡å‹")
        else:
            print(f"[Epoch {epoch+1}] âŒ éªŒè¯æŸå¤±æœªä¸‹é™ï¼šå½“å‰={avg_dev_loss:.4f} | æœ€ä¼˜={best_dev_loss:.4f}")

        print(f"[Epoch {epoch+1}] è®­ç»ƒæŸå¤±ï¼š{avg_train_loss:.4f} | éªŒè¯æŸå¤±ï¼š{avg_dev_loss:.4f}\n")

    print(f"===== è®­ç»ƒå®Œæˆï¼æœ€ä¼˜æ¨¡å‹ï¼š{SAVE_PATH} =====")

def infer(args):
    """æ¨ç†ï¼šæå–å½±è§†åç§°"""
    if not os.path.exists(SAVE_PATH):
        print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°æ¨¡å‹ {SAVE_PATH}ï¼Œè¯·å…ˆè®­ç»ƒï¼")
        return

    # åŠ è½½æ¨¡å‹
    model = MediaNameExtractor().to(DEVICE)
    try:
        model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
        return

    # æå–åç§°
    name = model.extract_name(args.path)
    return name

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å½±è§†åç§°æå–æ¨¡å‹ï¼ˆä¿®å¤æ¨ç†åˆ†æ•°å¼‚å¸¸ï¼‰")
    subparsers = parser.add_subparsers(dest="command", help="å­å‘½ä»¤ï¼štrain / infer")

    # è®­ç»ƒå­å‘½ä»¤
    train_parser = subparsers.add_parser("train", help="è®­ç»ƒæ¨¡å‹ï¼ˆè‡ªåŠ¨ç»­è®­ï¼‰")
    train_parser.add_argument("--train_data", type=str, default="train_data.txt", help="è®­ç»ƒæ•°æ®è·¯å¾„")
    train_parser.add_argument("--dev_data", type=str, default="dev_data.txt", help="éªŒè¯æ•°æ®è·¯å¾„")
    train_parser.add_argument("--epochs", type=int, default=10, help="è®­ç»ƒè½®æ•°ï¼ˆå»ºè®®10è½®ï¼‰")

    # æ¨ç†å­å‘½ä»¤
    infer_parser = subparsers.add_parser("infer", help="æå–å½±è§†åç§°")
    infer_parser.add_argument("--path", type=str, required=True, help="åŸå§‹æ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()
    if args.command == "train":
        train(args)
    elif args.command == "infer":
        infer(args)
    else:
        parser.print_help()